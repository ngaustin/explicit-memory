{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Union, Callable\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Transformer\n",
    "from torch.utils.data import dataset\n",
    "from memory.environment.generator import OQAGenerator\n",
    "from memory.environment.gym import MemoryEnv\n",
    "from memory import EpisodicMemory, SemanticMemory\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Turn a given memory table into learanble embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_params: dict,\n",
    "        embedding_dim: int,\n",
    "        num_rows: int,\n",
    "        num_cols: int,\n",
    "        special_tokens: dict = {\"<pad>\": 0, \"<mask>\": 1},\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize an Embeddings object.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        generator_params: OQAGenerator parameters\n",
    "        embedding_dim: embedding dimension (e.g., 4)\n",
    "        num_rows: number of rows in the table.\n",
    "        num_cols: number of columns in the table.\n",
    "        special_tokens: dict = {\"<pad>\": 0, \"<mask>\": 1}\n",
    "        device: \"cpu\" or \"cuda\"\n",
    "        sinusoidal_pos_embds: True if you want to use sinusoidal, False if you want to\n",
    "            use learnable.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        oqag = OQAGenerator(**generator_params)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.heads = nn.Embedding(len(oqag.heads), self.embedding_dim)\n",
    "        self.head2num = {head: idx for idx, head in enumerate(oqag.heads)}\n",
    "\n",
    "        self.relations = nn.Embedding(len(oqag.relations), self.embedding_dim)\n",
    "        self.relation2num = {\n",
    "            relation: idx for idx, relation in enumerate(oqag.relations)\n",
    "        }\n",
    "\n",
    "        self.tails = nn.Embedding(len(oqag.tails), self.embedding_dim)\n",
    "        self.tail2num = {tail: idx for idx, tail in enumerate(oqag.tails)}\n",
    "\n",
    "        self.names = nn.Embedding(len(oqag.names), self.embedding_dim)\n",
    "        self.name2num = {name: idx for idx, name in enumerate(oqag.names)}\n",
    "\n",
    "        self.specials = nn.Embedding(\n",
    "            len(special_tokens), self.embedding_dim, padding_idx=special_tokens[\"<pad>\"]\n",
    "        )\n",
    "        self.special2num = {token: num for token, num in special_tokens.items()}\n",
    "\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim*3)\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "    def episodic2numerics(self, M_e: EpisodicMemory) -> torch.Tensor:\n",
    "        \"\"\"Convert the EpisodicMemory object into numerical values.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        M_e: EpisodicMemory object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Episodic memory table whose shape is (num_memories, embedding_dim*3)\n",
    "\n",
    "        \"\"\"\n",
    "        table = []\n",
    "        for mem in M_e.entries:\n",
    "            head = mem[0]\n",
    "            relation = mem[1]\n",
    "            tail = mem[2]\n",
    "            # timestamps are not used at the moment since the memories are ordered by\n",
    "            # time anyways.\n",
    "            # timestamp = mem[3]\n",
    "\n",
    "            name1, head = M_e.split_name_entity(head)\n",
    "            name2, tail = M_e.split_name_entity(tail)\n",
    "\n",
    "            assert name1 == name2\n",
    "\n",
    "            head = self.names(\n",
    "                torch.tensor(self.name2num[name1], device=self.device)\n",
    "            ) + self.heads(torch.tensor(self.head2num[head], device=self.device))\n",
    "\n",
    "            relation = self.relations(\n",
    "                torch.tensor(self.relation2num[relation], device=self.device)\n",
    "            )\n",
    "\n",
    "            tail = self.names(\n",
    "                torch.tensor(self.name2num[name2], device=self.device)\n",
    "            ) + self.tails(torch.tensor(self.tail2num[tail], device=self.device))\n",
    "\n",
    "            table.append(torch.cat([head, relation, tail]))\n",
    "\n",
    "        table = torch.stack(table)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def eq2numerics(self, question: list) -> torch.Tensor:\n",
    "        \"\"\"Convert the question into numerical values.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        question: [head, relation]. E.g., [Tae's laptop, AtLocation]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Episodic question whose shape is (1, embedding_dim*3)\n",
    "\n",
    "        \"\"\"\n",
    "        head = question[0]\n",
    "        name, head = EpisodicMemory.split_name_entity(head)\n",
    "        relation = question[1]\n",
    "\n",
    "        head = self.names(\n",
    "            torch.tensor(self.name2num[name], device=self.device)\n",
    "        ) + self.heads(torch.tensor(self.head2num[head], device=self.device))\n",
    "\n",
    "        relation = self.relations(\n",
    "            torch.tensor(self.relation2num[relation], device=self.device)\n",
    "        )\n",
    "\n",
    "        tail = self.names(\n",
    "            torch.tensor(self.name2num[name], device=self.device)\n",
    "        ) + self.specials(torch.tensor(self.special2num[\"<mask>\"], device=self.device))\n",
    "\n",
    "        table = torch.cat([head, relation, tail]).unsqueeze(0)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def semantic2numerics(self, M_s: SemanticMemory) -> torch.Tensor:\n",
    "        \"\"\"Convert the SemanticMemory object into numerical values.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        M_s: SemanticMemory object.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Semantic memory table whose shape is (num_memories, embedding_dim*3)\n",
    "\n",
    "        \"\"\"\n",
    "        table = []\n",
    "        for mem in M_s.entries:\n",
    "            head = mem[0]\n",
    "            relation = mem[1]\n",
    "            tail = mem[2]\n",
    "\n",
    "            # num_gen are not used at the moment since the memories are ordered by\n",
    "            # num_gen anyways.\n",
    "            # num_gen = mem[3]\n",
    "\n",
    "            head = self.heads(torch.tensor(self.head2num[head], device=self.device))\n",
    "\n",
    "            relation = self.relations(\n",
    "                torch.tensor(self.relation2num[relation], device=self.device)\n",
    "            )\n",
    "\n",
    "            tail = self.tails(torch.tensor(self.tail2num[tail], device=self.device))\n",
    "\n",
    "            table.append(torch.cat([head, relation, tail]))\n",
    "\n",
    "        table = torch.stack(table)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def sq2numerics(self, question: list) -> torch.Tensor:\n",
    "        \"\"\"Convert the question into numerical values.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        question: [head, relation]. E.g., [Tae's laptop, AtLocation]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Semantic question whose shape is (1, embedding_dim*3)\n",
    "\n",
    "        \"\"\"\n",
    "        head = SemanticMemory.remove_name(question[0])\n",
    "        relation = question[1]\n",
    "\n",
    "        head = self.heads(torch.tensor(self.head2num[head], device=self.device))\n",
    "\n",
    "        relation = self.relations(\n",
    "            torch.tensor(self.relation2num[relation], device=self.device)\n",
    "        )\n",
    "\n",
    "        tail = self.specials(\n",
    "            torch.tensor(self.special2num[\"<mask>\"], device=self.device)\n",
    "        )\n",
    "\n",
    "        table = torch.cat([head, relation, tail]).unsqueeze(0)\n",
    "\n",
    "        return table\n",
    "\n",
    "    def forward(\n",
    "        self, M_e: EpisodicMemory, M_s: SemanticMemory, question: list, policy_type: str\n",
    "    ):\n",
    "        \"\"\"One forward pass.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        M_e: EpisodicMemory object.\n",
    "        M_s: SemanticMemory object.\n",
    "        question: [head, relation]. E.g., [Tae's laptop, AtLocation]\n",
    "        policy_type: one of the below:\n",
    "            episodic_memory_manage\n",
    "            episodic_question_answer\n",
    "            semantic_memory_manage\n",
    "            semantic_question_answer\n",
    "            episodic_to_semantic\n",
    "            episodic_semantic_question_answer\n",
    "\n",
    "        \"\"\"\n",
    "        x = torch.zeros(\n",
    "            self.num_rows, self.num_cols, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "\n",
    "        if policy_type == \"episodic_memory_manage\":\n",
    "            table = self.episodic2numerics(M_e)\n",
    "            x[: table.shape[0], :] = table\n",
    "\n",
    "        elif policy_type == \"episodic_question_answer\":\n",
    "            table = self.episodic2numerics(M_e)\n",
    "            x[: table.shape[0], :] = table\n",
    "\n",
    "            table = self.eq2numerics(question)\n",
    "            x[-1, :] = table\n",
    "\n",
    "        elif policy_type == \"semantic_memory_manage\":\n",
    "            table = self.semantic2numerics(M_s)\n",
    "            x[: table.shape[0], :] = table\n",
    "\n",
    "        elif policy_type == \"semantic_question_answer\":\n",
    "            table = self.semantic2numerics(M_s)\n",
    "            x[: table.shape[0], :] = table\n",
    "\n",
    "            table = self.sq2numerics(question)\n",
    "            x[-1, :] = table\n",
    "\n",
    "        elif policy_type == \"episodic_semantic_question_answer\":\n",
    "            table = self.episodic2numerics(M_e)\n",
    "            if len(table) > 0:\n",
    "                x[: table.shape[0], :] = table\n",
    "\n",
    "            table = self.semantic2numerics(M_s)\n",
    "            if len(table) > 0:\n",
    "                x[M_e.capacity : M_e.capacity + table.shape[0], :] = table\n",
    "\n",
    "            table = self.eq2numerics(question)\n",
    "            x[-1, :] = table\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        for idx in range(x.shape[0]):\n",
    "            x[idx] += self.positional_embeddings(torch.tensor(idx, device=self.device))\n",
    "\n",
    "        # x = x[torch.randperm(x.shape[0])]\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "transformer_params = {\n",
    "    \"d_model\": 4,\n",
    "    \"nhead\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"dim_feedforward\": 16,\n",
    "}\n",
    "\n",
    "transformer = Transformer(**transformer_params)\n",
    "\n",
    "src = torch.rand((5, 8, 4))\n",
    "tgt = torch.rand((1, 8, 4))\n",
    "out = transformer(src, tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Union, Callable\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from memory.environment.generator import OQAGenerator\n",
    "from memory.environment.gym import MemoryEnv\n",
    "\n",
    "transformer_params = {\n",
    "    \"d_model\": 4,\n",
    "    \"nhead\": 1,\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_encoder_layers\": 1,\n",
    "    \"num_decoder_layers\": 1,\n",
    "    \"dim_feedforward\": 16,\n",
    "}\n",
    "\n",
    "\n",
    "class MemoryTransformer(nn.Transformer):\n",
    "    def __init__(self, transformer_params: dict, generator_params: dict) -> None:\n",
    "\n",
    "        self.transformer_params = transformer_params\n",
    "        super().__init__(**self.transformer_params)\n",
    "        self.generator_params = generator_params\n",
    "        self.d_model = transformer_params[\"d_model\"]\n",
    "        self.dropout = transformer_params[\"dropout\"]\n",
    "        self.pos_encoder = PositionalEncoding(self.d_model, self.dropout)\n",
    "        oqag = OQAGenerator(**self.generator_params)\n",
    "\n",
    "        self.vocab = (\n",
    "            [\"<pad>\", \"<mask>\"]\n",
    "            + sorted(oqag.heads)\n",
    "            + sorted(oqag.relations)\n",
    "            + sorted(oqag.tails)\n",
    "        )\n",
    "        self.ntoken = len(self.vocab)\n",
    "        self.tokenizer = nn.Embedding(self.ntoken, self.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor,\n",
    "        tgt: Tensor,\n",
    "        src_mask: Optional[Tensor] = None,\n",
    "        tgt_mask: Optional[Tensor] = None,\n",
    "        memory_mask: Optional[Tensor] = None,\n",
    "        src_key_padding_mask: Optional[Tensor] = None,\n",
    "        tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "        memory_key_padding_mask: Optional[Tensor] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Take in and process masked source/target sequences.\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            src_mask: the additive mask for the src sequence (optional).\n",
    "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "            memory_mask: the additive mask for the encoder output (optional).\n",
    "            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
    "            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
    "        Shape:\n",
    "            - src: :math:`(S, E)` for unbatched input, :math:`(S, N, E)` if `batch_first=False` or\n",
    "              `(N, S, E)` if `batch_first=True`.\n",
    "            - tgt: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "            - src_mask: :math:`(S, S)`.\n",
    "            - tgt_mask: :math:`(T, T)`.\n",
    "            - memory_mask: :math:`(T, S)`.\n",
    "            - src_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "            - tgt_key_padding_mask: :math:`(T)` for unbatched input otherwise :math:`(N, T)`.\n",
    "            - memory_key_padding_mask: :math:`(S)` for unbatched input otherwise :math:`(N, S)`.\n",
    "            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
    "            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "            is provided, it will be added to the attention weight.\n",
    "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
    "            positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "            - output: :math:`(T, E)` for unbatched input, :math:`(T, N, E)` if `batch_first=False` or\n",
    "              `(N, T, E)` if `batch_first=True`.\n",
    "            Note: Due to the multi-head attention architecture in the transformer model,\n",
    "            the output sequence length of a transformer is same as the input sequence\n",
    "            (i.e. target) length of the decode.\n",
    "            where S is the source sequence length, T is the target sequence length, N is the\n",
    "            batch size, E is the feature number\n",
    "        Examples:\n",
    "            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        is_batched = src.dim() == 3\n",
    "        if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "        elif self.batch_first and src.size(0) != tgt.size(0) and is_batched:\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(-1) != self.d_model or tgt.size(-1) != self.d_model:\n",
    "            raise RuntimeError(\n",
    "                \"the feature number of src and tgt must be equal to d_model\"\n",
    "            )\n",
    "\n",
    "        memory = self.encoder(\n",
    "            src, mask=src_mask, src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        output = self.decoder(\n",
    "            tgt,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MemoryEnv()\n",
    "\n",
    "model = MemoryTransformer(transformer_params=transformer_params, generator_params=env.generator_params)\n",
    "\n",
    "src = torch.rand((5, 8, 4))\n",
    "tgt = torch.rand((1, 8, 4))\n",
    "out = model(src, tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.generator_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "from memory.environment.generator import OQAGenerator\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        d_hid: int,\n",
    "        nlayers: int,\n",
    "        generator_params: dict,\n",
    "        policy_type: str,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.generator_params = generator_params\n",
    "\n",
    "        oqag = OQAGenerator(**generator_params)\n",
    "\n",
    "        self.vocab = (\n",
    "            [\"<pad>\", \"<mask>\"]\n",
    "            + sorted(oqag.heads)\n",
    "            + sorted(oqag.relations)\n",
    "            + sorted(oqag.tails)\n",
    "        )\n",
    "\n",
    "        self.ntoken = len(self.vocab)\n",
    "\n",
    "        self.encoder = nn.Embedding(self.ntoken, d_model, padding_idx=0)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def tokenize(self) -> None:\n",
    "        oqag = OQAGenerator(**self.generator_params)\n",
    "        vocab = (\n",
    "            special_tokens\n",
    "            + sorted(oqag.heads)\n",
    "            + sorted(oqag.relations)\n",
    "            + sorted(oqag.tails)\n",
    "        )\n",
    "        ntoken = len(vocab)\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oqag = OQAGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(oqag.heads) + sorted(oqag.relations) + sorted(oqag.tails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(({\"<pad>\": 0, \"<mask>\": 1}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory.environment.gym import MemoryEnv\n",
    "\n",
    "env = MemoryEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(env.oqag.heads) + len(env.oqag.relations) + len(env.oqag.names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.generator_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.oqag.heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ntokens = 100\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "pos_encoder = PositionalEncoding(10, dropout, max_len=)\n",
    "for row in pos_encoder.pe.squeeze():\n",
    "    plt.plot(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(emsize, dropout)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(5):\n",
    "    plt.plot(pos_encoder(torch.tensor([i])).squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.triu(torch.ones(5, 5) * float(\"-inf\"), diagonal=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "embs = nn.Embedding(2, 3, padding_idx=0)\n",
    "fn = nn.Linear(3, 3)\n",
    "fn2 = nn.Linear(3, 3)\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=list(embs.parameters()) + list(fn.parameters()) + list(fn2.parameters())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    # loss = ((fn(embs(torch.tensor(1))) - torch.tensor([1,1,1]))**2).sum()\n",
    "    loss1 = ((fn(embs(torch.tensor(1))) - torch.tensor([2, 2, 2])) ** 2).sum()\n",
    "    loss2 = ((fn2(embs(torch.tensor(1))) - torch.tensor([1, 1, 1])) ** 2).sum()\n",
    "\n",
    "    loss = loss1 + loss2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(embs(torch.tensor(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs(torch.tensor(1)), fn.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn(embs(torch.tensor(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs(torch.tensor(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs(torch.tensor(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9f839f979dc5642b3e9e7f04449fa1fd1308ced902a4864cd6b54c390321673"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('dev-python3.7': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
