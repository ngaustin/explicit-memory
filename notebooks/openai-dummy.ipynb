{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/repos/explicit-memory\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from memory import Memory, EpisodicMemory, SemanticMemory\n",
    "from memory.environments import MemorySpace, EpisodicMemorySpace, SemanticMemorySpace, OQAGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEnv(gym.Env):\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        memory_type: str,\n",
    "        max_history: int = 1024,\n",
    "        semantic_knowledge_path: str = \"./data/semantic-knowledge.json\",\n",
    "        names_path: str = \"./data/top-human-names\",\n",
    "        weighting_mode: str = \"highest\",\n",
    "        commonsense_prob: float = 0.5,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        capacity: memory capacity\n",
    "        memory_type: either episodic or semantic.\n",
    "        max_history: maximum history of observations.\n",
    "        semantic_knowledge_path: path to the semantic knowledge generated from\n",
    "            `collect_data.py`\n",
    "        names_path: The path to the top 20 human name list.\n",
    "        weighting_mode: \"highest\" chooses the one with the highest weight, \"weighted\"\n",
    "            chooses all of them by weight, and null chooses every single one of them\n",
    "            without weighting.\n",
    "        commonsense_prob: the probability of an observation being covered by a\n",
    "            commonsense\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert memory_type in [\"episodic\", \"semantic\"]\n",
    "        self.memory_type = memory_type\n",
    "        self.capacity = capacity\n",
    "        self.oqag = OQAGenerator(\n",
    "            max_history,\n",
    "            semantic_knowledge_path,\n",
    "            names_path,\n",
    "            weighting_mode,\n",
    "            commonsense_prob,\n",
    "        )\n",
    "        n_actions = self.capacity + 1\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        self.observation_space = MemorySpace(\n",
    "            capacity,\n",
    "            memory_type,\n",
    "            max_history,\n",
    "            semantic_knowledge_path,\n",
    "            names_path,\n",
    "            weighting_mode,\n",
    "            commonsense_prob,\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.oqag.reset()\n",
    "        while True:\n",
    "            ob, question_answer = self.oqag.generate()\n",
    "            mem_epi = M_e.ob2epi(ob)\n",
    "\n",
    "\n",
    "        for idx, ob in enumerate(self.data[self.split]):\n",
    "            self.idx = idx\n",
    "            mem_epi = self.M_dummy.ob2epi(ob)\n",
    "            self.M_dummy.add(mem_epi)\n",
    "\n",
    "            if self.M_dummy.is_kinda_full:\n",
    "                break\n",
    "\n",
    "        assert idx == self.capacity\n",
    "\n",
    "        return self.M_dummy.entries\n",
    "\n",
    "    def step(self, action):\n",
    "        mem_to_forget = self.M_dummy.entries[action]\n",
    "\n",
    "        self.M_dummy.forget(mem_to_forget)\n",
    "\n",
    "        question = select_a_question(self.idx, self.data, self.questions, self.split)\n",
    "\n",
    "        reward, _, _ = self.M_dummy.answer_latest(question)\n",
    "\n",
    "        self.idx += 1\n",
    "        if self.idx == len(self.data[self.split]):\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "            ob = self.data[self.split][self.idx]\n",
    "            mem_epi = self.M_dummy.ob2epi(ob)\n",
    "            self.M_dummy.add(mem_epi)\n",
    "\n",
    "            assert self.M_dummy.is_kinda_full\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.M_dummy.entries, reward, done, info\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            print(self.M_dummy.entries)\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MemoryEnv('episodic', 4, 'train')\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MemoryEnv('episodic', 4, 'val')\n",
    "\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "n_steps = 1200\n",
    "\n",
    "for step in range(n_steps):\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    obs, reward, done, info = env.step(0)\n",
    "    print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory import EpisodicMemory\n",
    "\n",
    "capacities = [\n",
    "2,\n",
    "4, 0),\n",
    "    (8, 0),\n",
    "    (16, 0),\n",
    "    (32, 0),\n",
    "    (64, 0),\n",
    "    (128, 0),\n",
    "    (256, 0),\n",
    "    (512, 0),\n",
    "    (1024, 0),\n",
    "]\n",
    "\n",
    "results = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "\n",
    "    M_e = EpisodicMemory(capacity[\"episodic\"])\n",
    "    rewards = 0\n",
    "\n",
    "    for step, ob in enumerate(data[split]):\n",
    "        mem_epi = M_e.ob2epi(ob)\n",
    "        M_e.add(mem_epi)\n",
    "        if M_e.is_kinda_full:\n",
    "            if policy[\"forget\"].lower() == \"oldest\":\n",
    "                M_e.forget_oldest()\n",
    "            elif policy[\"forget\"].lower() == \"random\":\n",
    "                M_e.forget_random()\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        question = select_a_question(step, data, questions, split)\n",
    "\n",
    "        if policy[\"answer\"].lower() == \"latest\":\n",
    "            reward, _, _ = M_e.answer_latest(question)\n",
    "        elif policy[\"answer\"].lower() == \"random\":\n",
    "            reward, _, _ = M_e.answer_random(question)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        rewards += reward\n",
    "\n",
    "    results[split][\"rewards\"] = rewards\n",
    "    results[split][\"num_samples\"] = len(data[split])\n",
    "    results[split][\"episodic_memories\"] = M_e.entries\n",
    "\n",
    "    logging.info(f\"results so far: {results}\")\n",
    "\n",
    "logging.info(\"episodic only training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodic only\n",
    "\n",
    "capacity"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7808f786822013b9d5984aa54e12ef6bec326a79c76c0a75cd22ab652610adbd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('explicit-memory': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
