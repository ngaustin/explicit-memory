{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/repos/explicit-memory\n",
      "env: LOGLEVEL=ERROR\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.join(sys.path[0], \"..\"))\n",
    "os.environ[\"LOGLEVEL\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from memory.environment.generator import OQAGenerator\n",
    "from memory.environment.space import MemorySpace\n",
    "from memory import Memory, EpisodicMemory, SemanticMemory\n",
    "\n",
    "\n",
    "class EpisodicMemoryManageEnv(gym.Env):\n",
    "    \"\"\"Custom Memory environment compatiable with the gym interface.\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: dict,\n",
    "        max_history: int = 1024,\n",
    "        semantic_knowledge_path: str = \"./data/semantic-knowledge.json\",\n",
    "        names_path: str = \"./data/top-human-names\",\n",
    "        weighting_mode: str = \"highest\",\n",
    "        commonsense_prob: float = 0.5,\n",
    "        memory_manage: str = \"hand_crafted\",\n",
    "        question_answer: str = \"hand_crafted\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        capacity: memory capacity\n",
    "            e.g., {'episodic': 42, 'semantic: 0}\n",
    "        max_history: maximum history of observations.\n",
    "        semantic_knowledge_path: path to the semantic knowledge generated from\n",
    "            `collect_data.py`\n",
    "        names_path: The path to the top 20 human name list.\n",
    "        weighting_mode: \"highest\" chooses the one with the highest weight, \"weighted\"\n",
    "            chooses all of them by weight, and null chooses every single one of them\n",
    "            without weighting.\n",
    "        commonsense_prob: the probability of an observation being covered by a\n",
    "            commonsense\n",
    "        memory_manage: either \"hand_crafted\", \"RL_train\", or \"RL_trained\". Note that at\n",
    "            this point `memory_manage` and `question_answer` can't be \"RL\" at the same\n",
    "            time.\n",
    "        question_answer: either \"hand_crafted\", \"RL_trained\". Note that at\n",
    "            this point `memory_manage` and `question_answer` can't be \"RL\" at the same\n",
    "            time.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert memory_manage in [\"hand_crafted\", \"RL_train\", \"RL_trained\"]\n",
    "        assert question_answer in [\"hand_crafted\", \"RL_trained\"]\n",
    "        assert not (memory_manage == \"RL_train\" and question_answer == \"RL_train\")\n",
    "\n",
    "        self.memory_manage = memory_manage\n",
    "        self.question_answer = question_answer\n",
    "        assert capacity[\"semantic\"] == 0\n",
    "        self.capacity = capacity\n",
    "        self.oqag = OQAGenerator(\n",
    "            max_history,\n",
    "            semantic_knowledge_path,\n",
    "            names_path,\n",
    "            weighting_mode,\n",
    "            commonsense_prob,\n",
    "        )\n",
    "        self.n_actions = self.capacity[\"episodic\"] + 1\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.M_e = EpisodicMemory(self.capacity[\"episodic\"])\n",
    "        space_type = \"episodic_memory_manage\"\n",
    "\n",
    "        self.me_max = self.M_e.capacity + 1\n",
    "\n",
    "        self.observation_space = MemorySpace(\n",
    "            capacity,\n",
    "            space_type,\n",
    "            max_history,\n",
    "            semantic_knowledge_path,\n",
    "            names_path,\n",
    "            weighting_mode,\n",
    "            commonsense_prob,\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.oqag.reset()\n",
    "        self.M_e.forget_all()\n",
    "\n",
    "        return self.observation_space.episodic_memory_system_to_numbers(\n",
    "            self.M_e, self.me_max\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.M_e.is_empty:\n",
    "            ob, _ = self.generate_with_history(generate_qa=False)\n",
    "            mem_epi = self.M_e.ob2epi(ob)\n",
    "            self.M_e.add(mem_epi)\n",
    "\n",
    "        qa = self.oqag.generate_question_answer()\n",
    "\n",
    "        if self.question_answer == \"hand_crafted\":\n",
    "            reward, _, _ = self.M_e.answer_latest(qa)\n",
    "        elif self.question_answer == \"RL_trained\":\n",
    "            pass\n",
    "\n",
    "        if self.M_e.is_kinda_full:\n",
    "            if self.memory_manage == \"hand_crafted\":\n",
    "                self.M_e.forget_oldest()\n",
    "            elif self.question_answer == \"RL_train\":\n",
    "                mem = self.M_e.entries[action]\n",
    "                self.M_e.forget(mem)\n",
    "            elif self.question_answer == \"RL_trained\":\n",
    "                pass\n",
    "\n",
    "        if self.oqag.is_full:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        next_state = self.observation_space.episodic_memory_system_to_numbers(\n",
    "            self.M_e, self.me_max\n",
    "        )\n",
    "\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            print(self.M_e.entries)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(1025, 6)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, num_rows: int, num_cols: int):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        obs_size: observation/state size of the environment\n",
    "        n_actions: number of discrete actions available in the environment\n",
    "        hidden_size: size of hidden layers\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.LinearRow1 = nn.Linear(num_cols, num_cols)\n",
    "        self.LinearRow2 = nn.Linear(num_cols, 1)\n",
    "\n",
    "        self.LinearCol1 = nn.Linear(num_rows, num_rows)\n",
    "        self.LinearCol2 = nn.Linear(num_rows, num_rows)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.LinearRow1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.LinearRow2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = x.view(-1, self.num_rows)\n",
    "\n",
    "        x = self.LinearCol1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.LinearCol2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(\n",
    "            *(self.buffer[idx] for idx in indices)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self) -> Tuple:\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(\n",
    "            self.sample_size\n",
    "        )\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)\n",
    "            _, action = torch.max(q_values, dim=1)\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 1,\n",
    "        lr: float = 1e-2,\n",
    "        env: str = \"CartPole-v0\",\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate: int = 10,\n",
    "        replay_size: int = 1000,\n",
    "        warm_start_size: int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length: int = 200,\n",
    "        warm_start_steps: int = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "            replay_size: capacity of the replay buffer\n",
    "            warm_start_size: how many samples do we use to fill our buffer at the start of training\n",
    "            eps_last_frame: what frame should epsilon stop decaying\n",
    "            eps_start: starting value of epsilon\n",
    "            eps_end: final value of epsilon\n",
    "            episode_length: max length of an episode\n",
    "            warm_start_steps: max episode reward in the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = EpisodicMemoryManageEnv(capacity={\"episodic\": 1024, \"semantic\": 0})\n",
    "        # self.env = gym.make(self.hparams.env)\n",
    "        # self.env = EpisodicMemory(capacity=)\n",
    "        # obs_size = self.env.observation_space.shape[0]\n",
    "        # n_actions = self.env.action_space.n\n",
    "\n",
    "        # self.net = DQN(obs_size, n_actions)\n",
    "        # self.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "        self.net = DQN(1025, 6)\n",
    "        self.target_net = DQN(1025, 6)\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"Carries out several random steps through the environment to initially fill up the replay buffer with\n",
    "        experiences.\n",
    "\n",
    "        Args:\n",
    "            steps: number of random steps to populate the buffer with\n",
    "        \"\"\"\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        state_action_values = (\n",
    "            self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer.\n",
    "        Then calculates loss based on the minibatch recieved.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        batch: current mini batch of replay data\n",
    "        nb_batch: batch number\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Training loss and log metrics\n",
    "\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "            \"reward\": torch.tensor(reward).to(device),\n",
    "            \"train_loss\": loss,\n",
    "        }\n",
    "        print(log)\n",
    "        status = {\n",
    "            \"steps\": torch.tensor(self.global_step).to(device),\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "        }\n",
    "\n",
    "        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12044/3345317720.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNLightning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer = Trainer(\n\u001b[1;32m      4\u001b[0m     \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_12044/1681644050.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, lr, env, gamma, sync_rate, replay_size, warm_start_size, eps_last_frame, eps_start, eps_end, episode_length, warm_start_steps)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# self.target_net = DQN(obs_size, n_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "model = DQNLightning(batch_size=4)\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=0,\n",
    "    max_epochs=200,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7808f786822013b9d5984aa54e12ef6bec326a79c76c0a75cd22ab652610adbd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('explicit-memory': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
