{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5368192f-148a-42f0-966c-5624e788ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tk/repos/explicit-memory\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4e09a5-f063-4b05-9e82-4d54e1912dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict, deque, namedtuple\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.utilities import DistributedType\n",
    "from torch import Tensor, nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f107e5fd-a121-4c2e-af59-b63bcd0afb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_size: int, n_actions: int, hidden_size: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs_size: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11eb4b2-134c-45a6-b9b6-f701343fdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    \"Experience\",\n",
    "    field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2208c264-7a2e-43c2-9077-55995287247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(\n",
    "            *(self.buffer[idx] for idx in indices)\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff790979-9ced-4705-8d48-b295a365e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"Iterable Dataset containing the ExperienceBuffer which will be updated with new \n",
    "    experiences during training.\n",
    "\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: int = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self) -> Tuple:\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(\n",
    "            self.sample_size\n",
    "        )\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0cb5528-5222-4438-a974-4da1d230a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Base Agent class handeling the interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env, replay_buffer: ReplayBuffer) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            env: training environment\n",
    "            replay_buffer: replay buffer storing experiences\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.reset()\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resents the environment and updates the state.\"\"\"\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        \"\"\"Using the given network, decide what action to carry out using an \n",
    "        epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor([self.state])\n",
    "\n",
    "            if device not in [\"cpu\"]:\n",
    "                state = state.cuda(device)\n",
    "\n",
    "            q_values = net(state)\n",
    "            _, action = torch.max(q_values, dim=1)\n",
    "            action = int(action.item())\n",
    "\n",
    "        return action\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> Tuple[float, bool]:\n",
    "        \"\"\"Carries out a single interaction step between the agent and the environment.\n",
    "\n",
    "        Args:\n",
    "            net: DQN network\n",
    "            epsilon: value to determine likelihood of taking a random action\n",
    "            device: current device\n",
    "\n",
    "        Returns:\n",
    "            reward, done\n",
    "        \"\"\"\n",
    "\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "\n",
    "        self.replay_buffer.append(exp)\n",
    "\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95a0d76-30d8-42d8-8ebf-0a260dee9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(LightningModule):\n",
    "    \"\"\"Basic DQN Model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-2,\n",
    "        env: str = \"CartPole-v0\",\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate: int = 10,\n",
    "        replay_size: int = 1000,\n",
    "        warm_start_size: int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length: int = 200,\n",
    "        warm_start_steps: int = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size: size of the batches\")\n",
    "            lr: learning rate\n",
    "            env: gym environment tag\n",
    "            gamma: discount factor\n",
    "            sync_rate: how many frames do we update the target network\n",
    "            replay_size: capacity of the replay buffer\n",
    "            warm_start_size: how many samples do we use to fill our buffer at the start \n",
    "                of training\n",
    "            eps_last_frame: what frame should epsilon stop decaying\n",
    "            eps_start: starting value of epsilon\n",
    "            eps_end: final value of epsilon\n",
    "            episode_length: max length of an episode\n",
    "            warm_start_steps: max episode reward in the environment\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = gym.make(self.hparams.env)\n",
    "        obs_size = self.env.observation_space.shape[0]\n",
    "        n_actions = self.env.action_space.n\n",
    "\n",
    "        self.net = DQN(obs_size, n_actions)\n",
    "        self.target_net = DQN(obs_size, n_actions)\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "\n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"Carries out several random steps through the environment to initially fill up \n",
    "        the replay buffer with experiences.\n",
    "\n",
    "        Args:\n",
    "            steps: number of random steps to populate the buffer with\n",
    "        \"\"\"\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action \n",
    "        as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "    def dqn_mse_loss(self, batch: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "        \"\"\"Calculates the mse loss using a mini batch from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        state_action_values = (\n",
    "            self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], nb_batch) -> OrderedDict:\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer. \n",
    "        Then calculates loss based on the minibatch recieved.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            nb_batch: batch number\n",
    "\n",
    "        Returns:\n",
    "            Training loss and log metrics\n",
    "        \"\"\"\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if self.trainer._distrib_type in {DistributedType.DP, DistributedType.DDP2}:\n",
    "            loss = loss.unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "            \"reward\": torch.tensor(reward).to(device),\n",
    "            \"train_loss\": loss,\n",
    "        }\n",
    "        status = {\n",
    "            \"steps\": torch.tensor(self.global_step).to(device),\n",
    "            \"total_reward\": torch.tensor(self.total_reward).to(device),\n",
    "        }\n",
    "\n",
    "        return OrderedDict({\"loss\": loss, \"log\": log, \"progress_bar\": status})\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch.\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0782bf42-8bee-4888-859e-9c87415f6ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type | Params\n",
      "------------------------------------\n",
      "0 | net        | DQN  | 898   \n",
      "1 | target_net | DQN  | 898   \n",
      "------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/home/tk/.virtualenvs/explicit-memory/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 7it [00:00, 245.00it/s, loss=0.842, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/explicit-memory/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "/home/tk/.virtualenvs/explicit-memory/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:406: LightningDeprecationWarning: One of the returned values {'progress_bar', 'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  f\"One of the returned values {set(extra.keys())} has a `grad_fn`. We will detach it automatically\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: : 13it [00:00, 157.22it/s, loss=10.2, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "trainer = Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=1000,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0813fdf1-799a-49f4-8d72-512efb8c6429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9397b40989ccf5f4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9397b40989ccf5f4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd988cb-cf45-491c-86a5-cacee06ef70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7808f786822013b9d5984aa54e12ef6bec326a79c76c0a75cd22ab652610adbd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('explicit-memory': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
